{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_IDX = 5\n",
    "ROOT_DIR = r\"./dataWithPhoto/learning/fold{}/\".format(FOLD_IDX)\n",
    "VERSION = \"v4\"\n",
    "TRAIN_PATH = os.path.join(ROOT_DIR, r\"train/\")\n",
    "VALID_PATH = os.path.join(ROOT_DIR, r\"test/\")\n",
    "VALID_PRED_PATH = os.path.join(VALID_PATH, r\"pred-{}/\".format(VERSION))\n",
    "IMG_SHAPE = (512, 512, 3)\n",
    "LBL_SHAPE = IMG_SHAPE[:2]\n",
    "LOW_MEMORY = True\n",
    "USE_HSV_SPACE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, smooth=1.):\n",
    "    intersection = tf.reduce_sum(y_true*y_pred)\n",
    "    return 1-(2.*intersection)/(tf.reduce_sum(tf.square(y_true))+tf.reduce_sum(tf.square(y_pred))+smooth)\n",
    "\n",
    "def _circle_kernel_4D(radius):\n",
    "    _kernel = skimage.morphology.disk(radius, dtype=np.float32) - np.pad(skimage.morphology.disk(radius-1, dtype=np.float32), pad_width=1, mode='constant')\n",
    "    return _kernel[...,None,None] / _kernel.sum()\n",
    "\n",
    "Radii = [3,6,9,12,15,18]\n",
    "Kernels_in = [_circle_kernel_4D(r) for r in Radii]\n",
    "\n",
    "def HD_CV_loss(y_true, y_pred, kernels_in=Kernels_in, radii=Radii, alpha=2.0):\n",
    "    p = y_true\n",
    "    p_binary = tf.expand_dims(p, axis=-1) # 4D-tensor, shape=(1,512,512,1)\n",
    "    q = tf.clip_by_value(y_pred, 0., 1.)\n",
    "    q_binary = tf.expand_dims(tf.cast(tf.cast(q + 0.5, tf.int32), tf.float32), axis=-1) # 4D-tensor, shape=(1,512,512,1)\n",
    "    _squared_diff_pq = tf.square(p-q)\n",
    "    f_q_p = tf.expand_dims(tf.multiply(_squared_diff_pq, q), axis=-1) # 4D-tensor, shape=(1,512,512,1)\n",
    "    f_p_q = tf.expand_dims(tf.multiply(_squared_diff_pq, p), axis=-1) # 4D-tensor, shape=(1,512,512,1)\n",
    "    kernels = [tf.constant(k_in,dtype=tf.float32) for k_in in kernels_in]\n",
    "    loss_hd = 0.\n",
    "    for r,ker in zip(radii,kernels):\n",
    "        _out1 = tf.multiply(tf.nn.conv2d(1.-p_binary, ker, strides=1, padding=\"SAME\"), f_q_p)\n",
    "        _out2 = tf.multiply(tf.nn.conv2d(p_binary, ker, strides=1, padding=\"SAME\"), f_p_q)\n",
    "        _out3 = tf.multiply(tf.nn.conv2d(1.-q_binary, ker, strides=1, padding=\"SAME\"), f_p_q)\n",
    "        _out4 = tf.multiply(tf.nn.conv2d(q_binary, ker, strides=1, padding=\"SAME\"), f_q_p)\n",
    "        loss_hd += r**alpha * tf.reduce_mean(_out1+_out2+_out3+_out4)\n",
    "    return loss_hd\n",
    "\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "HD_Weight = 0.01\n",
    "BCE_Weight = 0.5\n",
    "\n",
    "def Dice_HD_compound_loss(y_true, y_pred, HD_Weight=HD_Weight):\n",
    "    return dice_loss(y_true, y_pred, smooth=1.) + HD_Weight * HD_CV_loss(y_true, y_pred, kernels_in=Kernels_in, radii=Radii, alpha=2.0)\n",
    "\n",
    "def Dice_BCE_compound_loss(y_true, y_pred, BCE_Weight=BCE_Weight):\n",
    "    return dice_loss(y_true, y_pred, smooth=1.) + BCE_Weight * bce_loss(y_true, y_pred)\n",
    "\n",
    "def Dice_HD_BCE_compound_loss(y_true, y_pred, HD_Weight=HD_Weight, BCE_Weight=BCE_Weight):\n",
    "    return dice_loss(y_true, y_pred, smooth=1.) + HD_Weight * HD_CV_loss(y_true, y_pred, kernels_in=Kernels_in, radii=Radii, alpha=2.0) + BCE_Weight * bce_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(parent_dir):\n",
    "    file_list = glob.glob(os.path.join(parent_dir, 'image', '*.png'))\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    for file_path in file_list:\n",
    "        label_path = os.path.join(parent_dir, 'label', os.path.basename(file_path))\n",
    "        image_list.append(skimage.io.imread(file_path))\n",
    "        label_list.append(skimage.io.imread(label_path))\n",
    "    return image_list, label_list\n",
    "\n",
    "def get_data_filenames(parent_dir):\n",
    "    image_filename_list = glob.glob(os.path.join(parent_dir, 'image', '*.png'))\n",
    "    label_filename_list = [os.path.join(parent_dir, 'label', os.path.basename(image_filename)) \\\n",
    "                          for image_filename in image_filename_list]\n",
    "    return image_filename_list, label_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_image(img):\n",
    "    assert img.ndim == 3, \"Input requires an RGB image\"\n",
    "    _max_val = np.max(img, axis=(0,1))\n",
    "    _min_val = np.min(img, axis=(0,1))\n",
    "    return (img - _min_val) / (_max_val - _min_val)\n",
    "\n",
    "def gen_data(image, label, train=True):\n",
    "    if train == True:\n",
    "        # random rotation\n",
    "        theta = 60 * (np.random.rand() - 0.5)\n",
    "        image = skimage.transform.rotate(image, theta)\n",
    "        label = skimage.transform.rotate(label, theta)\n",
    "        # horizontal flip or not\n",
    "        if np.random.rand() > 0.5:\n",
    "            image = image[:,::-1]\n",
    "            label = label[:,::-1]\n",
    "    # # increase constrast\n",
    "    # v_min, v_max = np.percentile(image, (0.1, 99.9))\n",
    "    # image = skimage.exposure.rescale_intensity(image, in_range=(v_min, v_max))\n",
    "\n",
    "    # resize to input shape (512,512)\n",
    "    image = skimage.transform.resize(image, IMG_SHAPE)\n",
    "    label = skimage.transform.resize(label, LBL_SHAPE) > 0.5\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, image_list, label_list, batch_size=8, train=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_list = image_list\n",
    "        self.label_list = label_list\n",
    "        self.train = train\n",
    "        self.index_cnt = len(self.image_list)\n",
    "        self.index_list = list(range(self.index_cnt))\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return self.index_cnt // self.batch_size\n",
    "    def __getitem__(self, k):\n",
    "        ks = k * self.batch_size\n",
    "        kt = ks + self.batch_size\n",
    "        x = np.empty((self.batch_size, *IMG_SHAPE), dtype='float32')\n",
    "        y = np.empty((self.batch_size, *LBL_SHAPE), dtype='float32')\n",
    "        for i, index in enumerate(range(ks, kt)):\n",
    "            real_index = self.index_list[index]\n",
    "            img = self.image_list[real_index]\n",
    "            lbl = self.label_list[real_index]\n",
    "            if LOW_MEMORY == True:\n",
    "                img = skimage.io.imread(img)\n",
    "                lbl = skimage.io.imread(lbl, as_gray=True)\n",
    "            if USE_HSV_SPACE == True:\n",
    "                img = skimage.color.rgb2hsv(img)\n",
    "            # dilation \n",
    "            lbl = skimage.morphology.dilation(lbl, skimage.morphology.disk(2)) # dilation edge prediction for visualization\n",
    "            x[i], y[i] = gen_data(img, lbl, self.train)\n",
    "        return x, y\n",
    "    def on_epoch_end(self):\n",
    "        if self.train == True:\n",
    "            np.random.shuffle(self.index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an almost typical U-Net 2015 (different in padding)\n",
    "def LeakyConv2D(x, filters, k_size=3, leaky_rate=.1, dila=1):\n",
    "    x = layers.Conv2D(filters, kernel_size=k_size, dilation_rate=dila, padding='same')(x)\n",
    "    x = layers.LeakyReLU(leaky_rate)(x)\n",
    "    return x\n",
    "\n",
    "def CascadeConv2D(x, filters, conv_times, k_size=3, leaky_rate=.1, dila=1):\n",
    "    for _ in range(conv_times):\n",
    "        x = LeakyConv2D(x, filters, k_size, leaky_rate, dila)\n",
    "    return x\n",
    "        \n",
    "def UNet2015(shape, kern_size=3, filters=[64,128,256,512,1024]):\n",
    "    outputShape = shape[:2] # (512,512)\n",
    "    encoders = []\n",
    "    inp = layers.Input(shape) # (512,512,3)\n",
    "    depth = 0\n",
    "    x = inp\n",
    "    conv_times = 2\n",
    "    for f in filters[:-1]:\n",
    "        x = CascadeConv2D(x, f, conv_times, kern_size, leaky_rate=.1, dila=1)\n",
    "        encoders.append(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        depth += 1\n",
    "    x = CascadeConv2D(x, filters[-1], conv_times, kern_size, leaky_rate=.1, dila=1)\n",
    "    while depth > 0 :\n",
    "        depth -= 1\n",
    "        f = filters[depth]\n",
    "        x = layers.Conv2DTranspose(f, kernel_size=2, strides=(2, 2), padding=\"valid\")(x)\n",
    "        x = layers.Concatenate()([x, encoders.pop()])\n",
    "        x = CascadeConv2D(x, f, conv_times, kern_size, leaky_rate=.1, dila=1)\n",
    "    x = LeakyConv2D(x, filters=1, k_size=1, leaky_rate=.1, dila=1)\n",
    "    x = layers.Reshape(outputShape)(x)\n",
    "    model = keras.Model(inp, x, name='UNet-2015')\n",
    "    return model\n",
    "\n",
    "def ResidualConv2D(x, filters, k_size=3, leaky_rate=.1, dila=1, batch_norm=True):\n",
    "    inx = layers.Conv2D(filters, kernel_size=1, strides=1, padding='same')(x)\n",
    "    for _ in range(2):\n",
    "        x = layers.Conv2D(filters, kernel_size=k_size, strides=1, dilation_rate=dila, padding='same')(x)\n",
    "        if batch_norm==True:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(leaky_rate)(x)\n",
    "    return layers.Add()([inx, x])\n",
    "\n",
    "def ResUNet(shape, kern_size=3, filters=[64,128,256,512,1024]):\n",
    "    outputShape = shape[:2] # (512,512)\n",
    "    encoders = []\n",
    "    inp = layers.Input(shape) # (512,512,3)\n",
    "    depth = 0\n",
    "    x = inp\n",
    "    for f in filters[:-1]:\n",
    "        x = ResidualConv2D(x, f, kern_size, leaky_rate=.1, dila=1)\n",
    "        encoders.append(x)\n",
    "        x = layers.MaxPooling2D(2)(x) + layers.AveragePooling2D(2)(x)\n",
    "        depth += 1\n",
    "    x = ResidualConv2D(x, filters[-1], kern_size, leaky_rate=.1, dila=1)\n",
    "    while depth > 0 :\n",
    "        depth -= 1\n",
    "        f = filters[depth]\n",
    "        x = layers.Conv2DTranspose(f, kernel_size=2, strides=(2, 2), padding=\"valid\")(x)\n",
    "        x = layers.Concatenate()([x, encoders.pop()])\n",
    "        x = ResidualConv2D(x, f, kern_size, leaky_rate=.1, dila=1)\n",
    "    x = LeakyConv2D(x, filters=1, k_size=1, leaky_rate=.1, dila=1)\n",
    "    x = layers.Reshape(outputShape)(x)\n",
    "    model = keras.Model(inp, x, name='ResUNet')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def ResConv2D(x, filters, k_size, conv_times, dial=1, leaky_rate=.1):\n",
    "    inx = layers.Conv2D(filters, kernel_size=1, strides=1, padding='same')(x)\n",
    "    for _ in range(conv_times):\n",
    "        x = LeakyConv2D(x, filters, k_size, leaky_rate, dial)\n",
    "    return layers.Add()([inx, x])\n",
    "\n",
    "def MultiAtrousConv2D(x, filters, kern_size=3, leaky_rate=.1, dila_rates=[1,2,4,8]):\n",
    "    out = []\n",
    "    for dila in dila_rates:\n",
    "        x = ResConv2D(x, filters, kern_size, conv_times=2, dial=dila, leaky_rate=.1)\n",
    "        out.append(x)\n",
    "    return layers.Concatenate()(out)\n",
    "\n",
    "def AtrousResUNet(shape, dila_rates=[1,2,4,8], filters=[4,8,16,32,64]):\n",
    "    outputShape = shape[:2] # (512,512)\n",
    "    encoders = []\n",
    "    inp = layers.Input(shape) # (512,512,3)\n",
    "    depth = 0\n",
    "    x = inp\n",
    "    kern_size = 3\n",
    "    for f in filters[:-1]:\n",
    "        x = MultiAtrousConv2D(x, f, kern_size, leaky_rate=.1, dila_rates=dila_rates)\n",
    "        encoders.append(x)\n",
    "        x = layers.MaxPooling2D(2)(x) + layers.AveragePooling2D(2)(x) \n",
    "        depth += 1\n",
    "    x = MultiAtrousConv2D(x, filters[-1], kern_size, leaky_rate=.1, dila_rates=dila_rates)\n",
    "    while depth > 0 :\n",
    "        depth -= 1\n",
    "        f = filters[depth]\n",
    "        x = layers.Conv2DTranspose(f, kernel_size=2, strides=(2, 2), padding=\"valid\")(x)\n",
    "        x = layers.Concatenate()([x, encoders.pop()])\n",
    "        x = MultiAtrousConv2D(x, f, kern_size, leaky_rate=.1, dila_rates=dila_rates)\n",
    "    x = ResConv2D(x, filters[-1], kern_size, conv_times=2, dial=1, leaky_rate=.1)\n",
    "    x = LeakyConv2D(x, 1, k_size=1, leaky_rate=.1, dila=1)\n",
    "    x = layers.Reshape(outputShape)(x)\n",
    "    model = keras.Model(inp, x, name='AtrousResUNet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet2015(IMG_SHAPE, kern_size=3, filters=[32,64,128,256]) \n",
    "\n",
    "# v2: 50 epochs with Dice loss\n",
    "# Fold 5 weights-dice-50-v2.h5 # dice loss 50 epochs # avg_dsc: 0.7130\n",
    "# Fold 4 weights-dice-50-v2.h5 # dice loss 50 epochs # avg_dsc: 0.7159\n",
    "# train-v2.log\n",
    "\n",
    "# v3: 5 epochs with Dice loss, 45 epochs with L_Dice + L_HD\n",
    "# train-v3.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AtrousResUNet(IMG_SHAPE, dila_rates=[1,2,4,8], filters=[4,8,16,32,64])\n",
    "# v4: 5 epochs with Dice loss, 45 epochs with L_Dice + L_HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code written by SHI Haochen\n",
    "def LeakyConv2D(x, filters, k_size, leaky_rate=.1, dila=1):\n",
    "    x = layers.Conv2D(filters, kernel_size=k_size, dilation_rate=dila, padding='same')(x)\n",
    "    x = layers.LeakyReLU(leaky_rate)(x)\n",
    "    return x\n",
    "\n",
    "def MultiConv2D(x, filters, k_size, conv_times, dial=1, leaky_rate=.1):\n",
    "    x = LeakyConv2D(x, filters, 1)\n",
    "    for _ in range(conv_times):\n",
    "        inx = x\n",
    "        x = LeakyConv2D(x, filters, k_size, leaky_rate, dial)\n",
    "        x = layers.Add()([inx, x])\n",
    "    return x\n",
    "\n",
    "def MultiDialConv2D(x, filters, k_size, conv_times, leaky_rate=.1):\n",
    "    out = []\n",
    "    for dial in [1, 5, 9]:\n",
    "        out.append(MultiConv2D(x, filters, k_size, conv_times, dial))\n",
    "    return layers.Concatenate()(out)\n",
    "\n",
    "def CreateUNet(shape, conv_times=2, filters=[4, 8, 16, 32, 64]):\n",
    "    depth = 0\n",
    "    encoders = []\n",
    "    inp = layers.Input(shape)\n",
    "    x = inp # layers.Reshape((*shape, 1))(inp)\n",
    "    for f in filters:\n",
    "        kern_size = 3\n",
    "        encoders.append(x)\n",
    "        x = MultiDialConv2D(x, f, kern_size, conv_times)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "        depth += 1\n",
    "    x = MultiDialConv2D(x, filters[-1], 3, conv_times-1)\n",
    "    x = LeakyConv2D(x, filters[-2], 3)\n",
    "    while depth > 0:\n",
    "        depth -= 1\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "        kern_size = 3\n",
    "        x = MultiDialConv2D(x, filters[depth], kern_size, conv_times)\n",
    "        x = layers.Concatenate()([x, encoders.pop()])\n",
    "    x = MultiDialConv2D(x, 1, 3, 2)\n",
    "    x = LeakyConv2D(x, 1, 3)\n",
    "    # x = layers.Lambda(lambda x: tf.abs(x))(x)\n",
    "    # x = layers.Softmax()(x)\n",
    "    outputShape = shape[:2]\n",
    "    x = layers.Reshape(outputShape)(x)\n",
    "    model = keras.Model(inp, x, name='UNet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CreateUNet(IMG_SHAPE, conv_times=2, filters=[4, 8, 16, 32, 64]) \n",
    "# v1\n",
    "# Fold 1\n",
    "# weights-dice.h5 # dice loss 10 epochs # avg_dsc: 0.6364\n",
    "# weights-dice-HD-10.h5 # dice+HD loss 10 epochs # avg_dsc: 0.6727\n",
    "# weights-dice-HD-20.h5 # dice+HD loss 20 epochs # avg_dsc: 0.6897\n",
    "# weights-dice-HD-30.h5 # dice+HD loss 30 epochs # avg_dsc: 0.7005\n",
    "# weights-dice-HD-80.h5 # dice+HD loss 80 epochs # avg_dsc: 0.7131\n",
    "\n",
    "# Fold 2\n",
    "# weights-dice.h5 # dice loss 10 epochs # avg_dsc: 0.6156\n",
    "# weights-dice-HD-10.h5 # dice+HD loss 10 epochs # avg_dsc: 0.6647\n",
    "# weights-dice-HD-20.h5 # dice+HD loss 20 epochs # avg_dsc: 0.6776\n",
    "# weights-dice-HD-30.h5 # dice+HD loss 30 epochs # avg_dsc: 0.6934\n",
    "# weights-dice-HD-80.h5 # dice+HD loss 80 epochs # avg_dsc: 0.7111\n",
    "\n",
    "# Fold 3\n",
    "# weights-dice.h5 # dice loss 10 epochs # avg_dsc: 0.6224\n",
    "# weights-dice-HD-30.h5 # dice+HD loss 30 epochs # avg_dsc: 0.6979\n",
    "# weights-dice-HD-80.h5 # dice+HD loss 80 epochs # avg_dsc: 0.7071\n",
    "\n",
    "# Fold 4\n",
    "# weights-dice.h5 # dice loss 10 epochs # avg_dsc: 0.6258\n",
    "# weights-dice-HD-20.h5 # dice+HD loss 20 epochs # avg_dsc: 0.6764\n",
    "# weights-dice-HD-80.h5 # dice+HD loss 80 epochs # avg_dsc: 0.7070\n",
    "\n",
    "# Fold 5\n",
    "# weights-dice.h5 # dice loss 10 epochs # avg_dsc: 0.6095\n",
    "# weights-dice-HD-20.h5 # dice+HD loss 20 epochs # avg_dsc: 0.6740\n",
    "# weights-dice-HD-80.h5 # dice+HD loss 80 epochs # avg_dsc: 0.7016\n",
    "\n",
    "# dial = [1,5,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = get_data_filenames(TRAIN_PATH)\n",
    "valid_image, valid_label = get_data_filenames(VALID_PATH)\n",
    "if not LOW_MEMORY: # 内存充足\n",
    "    train_image, train_label = read_data(TRAIN_PATH)\n",
    "    valid_image, valid_label = read_data(VALID_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dg = DataGenerator(train_image, train_label, batch_size, True)\n",
    "valid_dg = DataGenerator(valid_image, valid_label, batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=.0005), loss=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_ckpt = os.path.join(ROOT_DIR, r'weights-dice-5-{}.h5'.format(VERSION))\n",
    "print(weight_ckpt)\n",
    "model_checkpoint = ModelCheckpoint(weight_ckpt, monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True)\n",
    "ret = model.fit(x=train_dg, validation_data=valid_dg, epochs=5, verbose=1,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HD_Weight = 0.01\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=.0005), loss=Dice_HD_compound_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ckpt_to_load = os.path.join(ROOT_DIR, r'weights-dice-5-{}.h5'.format(VERSION))\n",
    "model.load_weights(weight_ckpt_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ckpt = os.path.join(ROOT_DIR, r'weights-dice-HD-45-{}.h5'.format(VERSION))\n",
    "print(weight_ckpt)\n",
    "model_checkpoint = ModelCheckpoint(weight_ckpt, monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True)\n",
    "ret = model.fit(x=train_dg, validation_data=valid_dg, epochs=45, verbose=1,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    largestCC = labels == np.argmax(np.bincount(labels.flat)[1:])+1 # 不考虑背景色\n",
    "    return largestCC\n",
    "\n",
    "def removeSmallCC(segmentation, minNumConnectedPixel):\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    labels2abandon = np.bincount(labels.flat) < minNumConnectedPixel\n",
    "    mask = labels2abandon[labels]\n",
    "    segmentation[mask] = 0.0\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # i-th mini-batch\n",
    "j = 0 # j-th image in mini-batch\n",
    "thre = 0.5\n",
    "init_img_shape = (1080,1440,3)\n",
    "init_lbl_shape = init_img_shape[:2]\n",
    "print(valid_image[i*batch_size+j])\n",
    "test_img, test_lbl = valid_dg[i]\n",
    "pred_lbl = model.predict(test_img)\n",
    "# pred_lbl = np.exp(pred_lbl) / (1.+np.exp(pred_lbl))\n",
    "_test_img = skimage.transform.resize(test_img[j], init_img_shape)\n",
    "_test_lbl = skimage.transform.resize(test_lbl[j], init_lbl_shape)\n",
    "_pred_lbl = skimage.transform.resize(pred_lbl[j], init_lbl_shape)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(131)\n",
    "plt.imshow(_test_img)\n",
    "plt.title(\"input photo\")\n",
    "plt.subplot(132)\n",
    "plt.imshow(_test_lbl)\n",
    "plt.title(\"edge mask ground truth\")\n",
    "plt.subplot(133)\n",
    "# _pred_lbl = skimage.morphology.erosion(_pred_lbl, skimage.morphology.disk(2))\n",
    "# plt.imshow(skimage.morphology.skeletonize(_pred_lbl>thre))\n",
    "# plt.imshow(removeSmallCC(_pred_lbl>thre, minNumConnectedPixel=100))\n",
    "plt.imshow(_pred_lbl>thre)\n",
    "plt.title(\"edge mask prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dsc(y, py):\n",
    "    _tf = np.count_nonzero(np.logical_and(y, py))\n",
    "    _sum = np.count_nonzero(y)+np.count_nonzero(py)\n",
    "    return 2*_tf/_sum\n",
    "\n",
    "def compute_avg_dsc(masks, pred_prob_map, thre=0.5, from_logits=False):\n",
    "    pred_masks = pred_prob_map.copy()\n",
    "    if from_logits == True:\n",
    "        pred_masks = np.exp(pred_masks) / (1.+np.exp(pred_masks))\n",
    "    pred_masks = pred_masks > thre\n",
    "    dsc_list = [calc_dsc(masks[i], pred_masks[i]) for i in range(len(masks))]\n",
    "    return np.mean(dsc_list)\n",
    "\n",
    "def save_pred_masks(pred_labels, file_names, mask_shape):\n",
    "    assert len(pred_labels) == len(file_names)\n",
    "    for i in range(len(file_names)):\n",
    "        pred_prob_map = skimage.transform.resize(pred_labels[i], mask_shape)\n",
    "        \n",
    "        pred_prob_map = skimage.morphology.erosion(pred_prob_map, skimage.morphology.disk(2))\n",
    "        # preserved_mask = removeSmallCC(pred_prob_map>0.5, minNumConnectedPixel=100)\n",
    "        # pred_prob_map = pred_prob_map * preserved_mask\n",
    "        # pred_prob_map[pred_prob_map<=0.5] = 0.\n",
    "        # pred_prob_map = np.clip(pred_prob_map, 0.0, 1.0)\n",
    "        \n",
    "        # pred_mask = skimage.morphology.skeletonize(pred_prob_map > 0.5)\n",
    "        # pred_mask = skimage.morphology.binary_closing(pred_mask)\n",
    "        \n",
    "        pred_mask = pred_prob_map > 0.5\n",
    "        pred_edge_img = (255. * pred_mask).astype(np.uint8)\n",
    "        skimage.io.imsave(file_names[i], pred_edge_img)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, save_pred_mask=False, mask_shape=(1080,1440)):\n",
    "    valid_image, valid_label = get_data_filenames(VALID_PATH)\n",
    "    if not LOW_MEMORY: # 内存充足\n",
    "        valid_image, valid_label = read_data(VALID_PATH)\n",
    "    valid_dg = DataGenerator(valid_image, valid_label, batch_size=1, train=False)\n",
    "    # evaluate\n",
    "    valid_pred_labels = model.predict(valid_dg)\n",
    "    valid_labels = np.concatenate([img_lbl_pair[1] for img_lbl_pair in valid_dg], axis=0)\n",
    "    valid_avg_dsc = compute_avg_dsc(valid_labels, valid_pred_labels, thre=0.5, from_logits=False)\n",
    "    print(\"Average DICE coefficient of validation data: {:.4f}\".format(valid_avg_dsc))\n",
    "    if save_pred_mask == True:\n",
    "        # save predicted edge mask\n",
    "        valid_img_names = glob.glob(os.path.join(VALID_PATH, 'image', '*.png'))\n",
    "        valid_pred_file_names = [os.path.join(VALID_PRED_PATH,os.path.basename(f)) for f in valid_img_names]\n",
    "        save_pred_masks(valid_pred_labels, valid_pred_file_names, mask_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(model, save_pred_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.6",
   "language": "python",
   "name": "tf2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
